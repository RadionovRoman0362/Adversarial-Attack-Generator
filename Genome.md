# Справочное Руководство по Компонентам Состязательной Атаки

Этот документ описывает все "строительные блоки" (компоненты), из которых алгоритм автоматически конструирует состязательные атаки. Каждая атака представляет собой "геном" — уникальную комбинацию этих компонентов из шести различных категорий.

## 1. Функции Потерь (`Loss`)

Функция потерь определяет **цель** атаки. Алгоритм пытается максимизировать это значение, чтобы "сломать" предсказание модели.

### `Cross-Entropy Loss`
*   **Описание:** Стандартная кросс-энтропия между логитами модели и истинными метками.
*   **Стратегия:** Заставить модель стать менее уверенной в правильном классе. Это базовая и самая распространенная стратегия.

### `Untargeted CW Loss`
*   **Описание:** Функция потерь Карлини-Вагнера. Максимизирует разницу между логитом правильного класса и максимальным логитом среди всех неправильных классов.
*   **Стратегия:** Более агрессивный подход. Атака не просто снижает уверенность в правильном ответе, а активно пытается сделать другой класс **более вероятным**, чем правильный.

### `Difference of Logits Ratio (DLR) Loss`
*   **Описание:** Улучшенная версия CW Loss, используемая в state-of-the-art атаке Auto-Attack.
*   **Стратегия:** Очень робастная и агрессивная стратегия, нацеленная на гарантированное изменение предсказания модели.

### `Popper Loss`
*   **Описание:** Максимизирует KL-дивергенцию между выходом модели и равномерным распределением по всем *неправильным* классам.
*   **Стратегия:** Заставить модель стать максимально "неуверенной" и "растерянной", распределив ее уверенность по всем возможным неправильным ответам.

## 2. Вычислители Градиента (`GradientComputer`)

Это ядро атаки, определяющее **направление** для изменения изображения. Компоненты этой категории делятся на два типа: **Терминальные** (завершают цепочку) и **Декораторы** (оборачивают другие компоненты).

### Терминальные Компоненты
Это базовые методы, которые непосредственно вычисляют градиент. Любая цепочка декораторов должна заканчиваться одним из этих компонентов.

#### `StandardGradient`
*   **Описание:** Вычисляет стандартный градиент функции потерь по входному изображению (`loss.backward()`).
*   **Стратегия:** Самый прямой и простой способ найти направление для атаки.

#### `Input Diversity Gradient (DI)`
*   **Описание:** Перед вычислением градиента применяет к входному изображению случайные трансформации (изменение размера, паддинг).
*   **Стратегия:** Повышение **переносимости** (transferability). Атака, сгенерированная этим методом, с большей вероятностью обманет другие модели, так как она не "переобучается" на специфические признаки одной суррогатной модели.

#### `Transferable Adversarial Perturbations (TAP)`
*   **Описание:** Использует комбинированную функцию потерь, которая одновременно максимизирует ошибку на суррогатной модели и заставляет ее предсказания **расходиться** с предсказаниями других моделей в ансамбле.
*   **Стратегия:** Явно оптимизирует атаку на **переносимость**. Идеально подходит для атак на черные ящики и ансамбли.

### Декораторы
Эти компоненты "оборачивают" другие вычислители градиента (включая другие декораторы), модифицируя их поведение.

#### `Momentum Gradient`
*   **Описание:** Накапливает градиенты с предыдущих шагов, используя скользящее среднее.
*   **Стратегия:** Стабилизирует направление атаки, помогая ей "перепрыгивать" через плохие локальные оптимумы и двигаться в более стабильном направлении. Значительно повышает эффективность атаки.

#### `Adversarial Gradient Smoothing (SmoothGrad)`
*   **Описание:** Вычисляет градиент не для одного изображения, а для нескольких его зашумленных копий, и усредняет результат.
*   **Стратегия:** Создает более робастную и стабильную оценку градиента, что может улучшить как силу атаки, так и ее переносимость.

#### `Translation-Invariant Gradient (TI)`
*   **Описание:** Применяет Гауссову свертку (размытие) к уже вычисленному градиенту.
*   **Стратегия:** Повышение **переносимости**. Размытие градиента делает атаку менее чувствительной к точному положению объектов на изображении, что помогает обманывать модели с разными архитектурами (например, Vision Transformers).

#### `Ensemble Gradient`
*   **Описание:** Усредняет градиенты, вычисленные на нескольких моделях.
*   **Стратегия:** Классический и очень мощный метод для повышения **переносимости**. Атака ищет уязвимости, общие для целого ансамбля моделей.

#### `Skip Gradient Method (SGM)`
*   **Описание:** Во время обратного прохода градиента "выпрямляет" нелинейные функции активации ReLU, пропуская градиент через них без изменений.
*   **Стратегия:** Повышение **переносимости**. Позволяет обойти специфичные для модели пути активации и защитные механизмы, основанные на нелинейностях.

## 3. Инициализаторы (`Initializer`)

Определяет **стартовую точку** атаки.

### `Zero Initializer`
*   **Описание:** Атака начинается непосредственно с чистого, оригинального изображения.
*   **Стратегия:** Базовый вариант.

### `Random L-infinity / L2 Initializer`
*   **Описание:** Атака начинается со случайной точки в пределах разрешенной `epsilon`-окрестности.
*   **Стратегия:** Стандарт для мощных атак (например, PGD). Помогает избежать плохих локальных оптимумов и значительно повышает вероятность успеха.

## 4. Правила Обновления (`UpdateRule`)

Определяет, **как именно** градиент и размер шага применяются для модификации изображения.

### `Standard Update`
*   **Описание:** `x_new = x + step_size * gradient`.
*   **Стратегия:** Простой шаг по направлению градиента.

### `Sign Update`
*   **Описание:** `x_new = x + step_size * sign(gradient)`.
*   **Стратегия:** Стандарт для L-infinity атак (FGSM, PGD). Делает шаг максимальной величины по каждому пикселю.

### `L2 Update`
*   **Описание:** `x_new = x + step_size * normalized(gradient)`.
*   **Стратегия:** Стандарт для L2 атак. Делает шаг фиксированной евклидовой длины.

### `Adam Update`
*   **Описание:** Использует логику оптимизатора Adam для адаптивного подбора шага для каждого пикселя.
*   **Стратегия:** Может быть эффективен для сложных ландшафтов потерь.

## 5. Планировщики (`Scheduler`)

Управляет **размером шага** на протяжении итераций атаки.

*   **`Fixed`:** Постоянный размер шага.
*   **`Linear Decay`:** Линейное уменьшение шага до нуля.
*   **`Cosine Annealing`:** Уменьшение шага по косинусоиде.
*   **`Multi-Step Decay`:** Уменьшение шага в несколько раз на заданных итерациях.
*   **`Plateau Reduce`:** Уменьшает шаг, если атака перестает улучшаться (выходит на плато).
*   **`Adaptive`:** Адаптивно изменяет шаг на основе истории потерь (похоже на логику Auto-PGD).

## 6. Проекторы (`Projector`)

Гарантирует, что атака **остается в допустимых границах** (в `epsilon`-окрестности).

*   **`L-infinity Projector`:** Ограничивает максимальное изменение каждого пикселя.
*   **`L2 Projector`:** Ограничивает евклидову норму всего вектора возмущения.
*   **`L1 / L0 Projectors`:** Ограничивают L1/L0 норму, что способствует созданию разреженных (sparse) атак, где изменено малое количество пикселей.